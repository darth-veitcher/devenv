# Pragmatic Python Package Architecture: A Definitive Guide

Use the following guidance when developing Python packages. This is a comprehensive, opinionated architecture that clearly separates concerns and prevents common pitfalls - particularly logic bleeding into the CLI and insufficient documentation.

## Core Architectural Principles

### The Palantir Three-Layer Ontology Applied

1. **Semantic Layer** (What things ARE) → Domain models, value objects, entities
2. **Kinetic Layer** (What things DO) → Services, business logic, workflows
3. **Dynamic Layer** (How things CHANGE) → Adapters, external integrations, state management

### The Cardinal Rule

> **The CLI is a thin shell that translates user intent into service calls. It contains ZERO business logic.**

---

## Package Structure

```
{{package_name}}/
├── pyproject.toml
├── noxfile.py
├── Dockerfile
├── docker-compose.yml
├── .devcontainer/
│   └── devcontainer.json
├── .github/
│   └── workflows/
│       ├── ci.yml
│       ├── release.yml
│       └── docs.yml
├── docs/
│   ├── mkdocs.yml
│   ├── index.md
│   ├── getting-started/
│   │   ├── installation.md
│   │   ├── quickstart.md
│   │   └── configuration.md
│   ├── concepts/
│   │   ├── architecture.md
│   │   ├── domain-model.md
│   │   └── workflows.md
│   ├── guides/
│   │   ├── cli-usage.md
│   │   ├── api-integration.md
│   │   └── extending.md
│   ├── reference/
│   │   ├── api/
│   │   │   └── index.md          # mkdocstrings auto-generated
│   │   ├── cli/
│   │   │   └── index.md
│   │   └── configuration.md
│   └── changelog.md
├── src/
│   └── {{package_name}}/
│       ├── __init__.py           # Package metadata, version
│       ├── py.typed               # PEP 561 marker
│       │
│       ├── _version.py           # Generated by hatch-vcs
│       │
│       ├── cli/                   # LAYER: Interface (Thin Shell)
│       │   ├── __init__.py
│       │   ├── main.py            # Typer app, command groups
│       │   ├── commands/          # Command implementations
│       │   │   ├── __init__.py
│       │   │   ├── config.py      # Config management commands
│       │   │   ├── admin.py       # Admin/maintenance commands
│       │   │   └── run.py         # Core workflow commands
│       │   ├── formatters.py      # Output formatting (table, json, etc.)
│       │   └── callbacks.py       # Shared CLI callbacks (verbosity, etc.)
│       │
│       ├── domain/                # LAYER: Semantic (What things ARE)
│       │   ├── __init__.py
│       │   ├── entities/          # Core business entities
│       │   │   ├── __init__.py
│       │   │   └── *.py
│       │   ├── value_objects/     # Immutable domain values
│       │   │   ├── __init__.py
│       │   │   └── *.py
│       │   ├── enums.py           # Domain enumerations
│       │   ├── events.py          # Domain events
│       │   └── exceptions.py      # Domain-specific exceptions
│       │
│       ├── models/                # LAYER: Data Transfer/Persistence
│       │   ├── __init__.py
│       │   ├── schemas/           # Pydantic models for validation
│       │   │   ├── __init__.py
│       │   │   ├── requests.py    # Input validation models
│       │   │   ├── responses.py   # Output serialization models
│       │   │   └── internal.py    # Internal DTOs
│       │   └── orm/               # SQLAlchemy/DB models (if needed)
│       │       ├── __init__.py
│       │       └── *.py
│       │
│       ├── services/              # LAYER: Kinetic (What things DO)
│       │   ├── __init__.py
│       │   ├── base.py            # Abstract service interfaces
│       │   ├── {{core_service}}.py # Primary business logic
│       │   └── orchestrator.py    # Workflow coordination
│       │
│       ├── adapters/              # LAYER: Dynamic (External Integration)
│       │   ├── __init__.py
│       │   ├── protocols.py       # Abstract interfaces (Protocols)
│       │   ├── http/              # HTTP client adapters
│       │   │   ├── __init__.py
│       │   │   └── *.py
│       │   ├── storage/           # Storage adapters (filesystem, S3, etc.)
│       │   │   ├── __init__.py
│       │   │   └── *.py
│       │   ├── database/          # Database adapters
│       │   │   ├── __init__.py
│       │   │   └── *.py
│       │   └── messaging/         # Queue/message adapters
│       │       ├── __init__.py
│       │       └── *.py
│       │
│       ├── api/                   # LAYER: Interface (HTTP API)
│       │   ├── __init__.py
│       │   ├── app.py             # FastAPI application factory
│       │   ├── dependencies.py    # Dependency injection
│       │   ├── middleware.py      # Custom middleware
│       │   └── routes/            # API route handlers
│       │       ├── __init__.py
│       │       ├── health.py
│       │       └── v1/
│       │           ├── __init__.py
│       │           └── *.py
│       │
│       ├── config/                # LAYER: Cross-Cutting
│       │   ├── __init__.py
│       │   ├── settings.py        # Pydantic Settings models
│       │   ├── defaults.py        # Default configuration values
│       │   └── loaders.py         # Config file loaders
│       │
│       ├── logging/               # LAYER: Cross-Cutting
│       │   ├── __init__.py
│       │   ├── setup.py           # Logger configuration
│       │   ├── formatters.py      # Custom log formatters
│       │   ├── handlers.py        # Custom handlers
│       │   └── context.py         # Structured logging context
│       │
│       └── utils/                 # LAYER: Cross-Cutting (Shared utilities)
│           ├── __init__.py
│           ├── async_helpers.py   # Async utilities
│           ├── retry.py           # Retry/backoff logic
│           └── hashing.py         # Content hashing utilities
│
└── tests/
    ├── __init__.py
    ├── conftest.py                # Shared fixtures
    ├── fixtures/                  # Test data files
    │   └── *.json
    ├── unit/                      # Unit tests (fast, isolated)
    │   ├── __init__.py
    │   ├── domain/
    │   ├── models/
    │   ├── services/
    │   └── adapters/
    ├── integration/               # Integration tests (real dependencies)
    │   ├── __init__.py
    │   └── *.py
    └── e2e/                       # End-to-end tests (full stack)
        ├── __init__.py
        └── *.py
```

---

## Layer Definitions and Responsibilities

### 1. CLI Layer (`cli/`)

**Purpose:** Thin interface that translates human commands into service invocations.

**MUST:**
- Parse and validate command-line arguments
- Configure logging verbosity
- Format output for human consumption (tables, JSON, etc.)
- Handle keyboard interrupts gracefully
- Delegate ALL work to services

**MUST NOT:**
- Contain business logic
- Directly call adapters (only through services)
- Perform data transformations beyond output formatting
- Handle retries, caching, or orchestration

```python
# src/{{package_name}}/cli/commands/run.py
"""
CLI commands for executing core workflows.

This module provides the command-line interface for triggering
the primary package workflows. All business logic is delegated
to the service layer.

Example:
    ```bash
    # Run the primary workflow
    {{package_name}} run process --input data.json --output results/

    # Run with verbose logging
    {{package_name}} -v run process --input data.json
    ```
"""

from typing import Annotated, Optional
from pathlib import Path

import typer
from rich.console import Console

from {{package_name}}.services.orchestrator import WorkflowOrchestrator
from {{package_name}}.config.settings import Settings
from {{package_name}}.cli.formatters import format_result
from {{package_name}}.logging.setup import get_logger

logger = get_logger(__name__)
console = Console()
app = typer.Typer(help="Core workflow commands")


@app.command()
def process(
    input_path: Annotated[
        Path,
        typer.Option(
            "--input", "-i",
            help="Path to input data file",
            exists=True,
            readable=True,
        ),
    ],
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output", "-o",
            help="Directory for output results",
        ),
    ],
    dry_run: Annotated[
        bool,
        typer.Option(
            "--dry-run",
            help="Simulate execution without writing results",
        ),
    ] = False,
) -> None:
    """
    Process input data through the primary workflow.

    This command orchestrates the complete processing pipeline,
    reading from INPUT_PATH and writing results to OUTPUT_DIR.

    Args:
        input_path: Path to the input data file (JSON, YAML, or CSV).
        output_dir: Directory where results will be written.
        dry_run: If True, simulate execution without side effects.

    Example:
        ```python
        # Programmatic equivalent (for testing)
        from {{package_name}}.cli.commands.run import process
        from typer.testing import CliRunner

        runner = CliRunner()
        result = runner.invoke(process, ["--input", "data.json", "--output", "out/"])
        assert result.exit_code == 0
        ```

    <!-- Example Test:
    >>> from pathlib import Path
    >>> from {{package_name}}.cli.commands.run import process
    >>> # This is a CLI command, tested via integration tests
    >>> assert callable(process)
    -->
    """
    # CLI responsibility: Setup and delegation
    settings = Settings()
    orchestrator = WorkflowOrchestrator(settings=settings)

    with console.status("[bold green]Processing..."):
        # Delegate to service layer - NO business logic here
        result = orchestrator.execute_workflow(
            input_path=input_path,
            output_dir=output_dir,
            dry_run=dry_run,
        )

    # CLI responsibility: Output formatting
    console.print(format_result(result))

    if result.success:
        raise typer.Exit(code=0)
    else:
        raise typer.Exit(code=1)
```

---

### 2. Domain Layer (`domain/`)

**Purpose:** Core business concepts that exist independently of any technical implementation.

**MUST:**
- Define entities with identity and lifecycle
- Define immutable value objects
- Define domain events and exceptions
- Be completely free of external dependencies (no I/O, no frameworks)
- Use only Python stdlib + typing

**MUST NOT:**
- Import from adapters, services, or API layers
- Contain persistence logic
- Reference configuration
- Perform I/O operations

```python
# src/{{package_name}}/domain/entities/document.py
"""
Document entity representing a processable unit of content.

This module defines the core Document entity which serves as the
fundamental unit of work within the package's domain model.

The Document entity follows Domain-Driven Design principles:
- Has a unique identity (UUID)
- Has a lifecycle (created, processed, archived)
- Encapsulates domain behavior and invariants

Example:
    ```python
    from {{package_name}}.domain.entities.document import Document
    from {{package_name}}.domain.value_objects.content import Content

    content = Content(text="Hello, world!", mime_type="text/plain")
    doc = Document.create(content=content, source="user_upload")
    print(f"Document ID: {doc.id}")
    ```
"""

from __future__ import annotations

import hashlib
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Optional

from {{package_name}}.domain.enums import DocumentStatus
from {{package_name}}.domain.value_objects.content import Content
from {{package_name}}.domain.exceptions import DomainValidationError


@dataclass
class Document:
    """
    Core domain entity representing a processable document.

    A Document is the fundamental unit of work in the system. It has
    identity, state, and behavior defined by business rules.

    Attributes:
        id: Unique identifier (UUID5 derived from content hash for idempotency).
        content: The document's content as a value object.
        source: Origin of the document (e.g., "api", "cli", "scheduled").
        status: Current lifecycle status.
        created_at: Timestamp of creation.
        processed_at: Timestamp of processing completion (if applicable).
        metadata: Additional key-value metadata.

    Example:
        ```python
        from {{package_name}}.domain.entities.document import Document
        from {{package_name}}.domain.value_objects.content import Content

        content = Content(text="Sample content", mime_type="text/plain")
        doc = Document.create(content=content, source="test")

        assert doc.status == DocumentStatus.PENDING
        assert doc.id is not None
        ```

    <!-- Example Test:
    >>> from {{package_name}}.domain.entities.document import Document
    >>> from {{package_name}}.domain.value_objects.content import Content
    >>> from {{package_name}}.domain.enums import DocumentStatus
    >>> content = Content(text="Test", mime_type="text/plain")
    >>> doc = Document.create(content=content, source="test")
    >>> assert isinstance(doc.id, uuid.UUID)
    >>> assert doc.status == DocumentStatus.PENDING
    >>> assert doc.source == "test"
    -->
    """

    id: uuid.UUID
    content: Content
    source: str
    status: DocumentStatus = DocumentStatus.PENDING
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    processed_at: Optional[datetime] = None
    metadata: dict[str, str] = field(default_factory=dict)

    # Namespace UUID for deterministic ID generation
    _NAMESPACE = uuid.UUID("6ba7b810-9dad-11d1-80b4-00c04fd430c8")

    @classmethod
    def create(
        cls,
        content: Content,
        source: str,
        metadata: Optional[dict[str, str]] = None,
    ) -> Document:
        """
        Factory method for creating new Document instances.

        Uses content-addressable ID generation (UUID5) to ensure
        idempotent document creation - the same content always
        produces the same document ID.

        Args:
            content: The document content as a value object.
            source: Origin identifier for the document.
            metadata: Optional additional metadata.

        Returns:
            A new Document instance with a deterministic ID.

        Raises:
            DomainValidationError: If content or source is invalid.

        Example:
            ```python
            from {{package_name}}.domain.entities.document import Document
            from {{package_name}}.domain.value_objects.content import Content

            content = Content(text="Hello", mime_type="text/plain")
            doc1 = Document.create(content=content, source="test")
            doc2 = Document.create(content=content, source="test")

            # Same content = same ID (idempotent)
            assert doc1.id == doc2.id
            ```

        <!-- Example Test:
        >>> from {{package_name}}.domain.entities.document import Document
        >>> from {{package_name}}.domain.value_objects.content import Content
        >>> c = Content(text="Deterministic", mime_type="text/plain")
        >>> d1 = Document.create(content=c, source="a")
        >>> d2 = Document.create(content=c, source="a")
        >>> assert d1.id == d2.id
        -->
        """
        if not source or not source.strip():
            raise DomainValidationError("Document source cannot be empty")

        # Deterministic ID from content hash
        content_hash = hashlib.sha256(content.text.encode()).hexdigest()
        doc_id = uuid.uuid5(cls._NAMESPACE, f"{content_hash}:{source}")

        return cls(
            id=doc_id,
            content=content,
            source=source.strip(),
            metadata=metadata or {},
        )

    def mark_processed(self) -> None:
        """
        Transition document to processed status.

        This method enforces the domain invariant that only
        PENDING or PROCESSING documents can be marked as processed.

        Raises:
            DomainValidationError: If the document is not in a valid state.

        Example:
            ```python
            from {{package_name}}.domain.entities.document import Document
            from {{package_name}}.domain.value_objects.content import Content

            content = Content(text="Process me", mime_type="text/plain")
            doc = Document.create(content=content, source="test")
            doc.mark_processed()

            assert doc.status == DocumentStatus.PROCESSED
            assert doc.processed_at is not None
            ```

        <!-- Example Test:
        >>> from {{package_name}}.domain.entities.document import Document
        >>> from {{package_name}}.domain.value_objects.content import Content
        >>> from {{package_name}}.domain.enums import DocumentStatus
        >>> c = Content(text="Test", mime_type="text/plain")
        >>> d = Document.create(content=c, source="test")
        >>> d.mark_processed()
        >>> assert d.status == DocumentStatus.PROCESSED
        >>> assert d.processed_at is not None
        -->
        """
        valid_transitions = {DocumentStatus.PENDING, DocumentStatus.PROCESSING}
        if self.status not in valid_transitions:
            raise DomainValidationError(
                f"Cannot mark document as processed from status: {self.status}"
            )

        self.status = DocumentStatus.PROCESSED
        self.processed_at = datetime.now(timezone.utc)
```

---

### 3. Models Layer (`models/`)

**Purpose:** Data transfer objects (DTOs) for validation, serialization, and persistence.

**MUST:**
- Define Pydantic models for input validation
- Define response models for API serialization
- Provide mapping functions to/from domain entities
- Define ORM models if using a database

**MUST NOT:**
- Contain business logic
- Be used directly in domain layer
- Make external calls

```python
# src/{{package_name}}/models/schemas/requests.py
"""
Request validation models for API and CLI input.

This module defines Pydantic models used to validate and parse
incoming data from external sources (API requests, CLI arguments,
configuration files).

These models act as the boundary validation layer, ensuring all
data entering the system conforms to expected shapes and constraints.

Example:
    ```python
    from {{package_name}}.models.schemas.requests import ProcessRequest

    # Valid request
    request = ProcessRequest(
        content="Hello, world!",
        source="api",
        options={"format": "json"},
    )

    # Invalid request raises ValidationError
    try:
        ProcessRequest(content="", source="api")  # Empty content
    except ValidationError as e:
        print(e.errors())
    ```
"""

from __future__ import annotations

from typing import Any, Optional

from pydantic import BaseModel, Field, field_validator


class ProcessRequest(BaseModel):
    """
    Request model for document processing operations.

    This model validates incoming processing requests and provides
    a clean interface for the service layer to consume.

    Attributes:
        content: The text content to process. Must be non-empty.
        source: Identifier for the request origin.
        mime_type: Content MIME type. Defaults to text/plain.
        options: Optional processing options as key-value pairs.

    Example:
        ```python
        from {{package_name}}.models.schemas.requests import ProcessRequest

        request = ProcessRequest(
            content="Process this text",
            source="cli",
            mime_type="text/plain",
            options={"verbose": True},
        )
        print(f"Processing {len(request.content)} characters from {request.source}")
        ```

    <!-- Example Test:
    >>> from {{package_name}}.models.schemas.requests import ProcessRequest
    >>> req = ProcessRequest(content="Test", source="unit_test")
    >>> assert req.content == "Test"
    >>> assert req.source == "unit_test"
    >>> assert req.mime_type == "text/plain"
    -->
    """

    content: str = Field(
        ...,
        min_length=1,
        max_length=1_000_000,
        description="Text content to process",
    )
    source: str = Field(
        ...,
        min_length=1,
        max_length=100,
        description="Origin identifier for the request",
    )
    mime_type: str = Field(
        default="text/plain",
        description="MIME type of the content",
    )
    options: dict[str, Any] = Field(
        default_factory=dict,
        description="Additional processing options",
    )

    @field_validator("content")
    @classmethod
    def content_not_whitespace(cls, v: str) -> str:
        """Ensure content is not just whitespace."""
        if not v.strip():
            raise ValueError("Content cannot be empty or whitespace only")
        return v

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "content": "Hello, world!",
                    "source": "api",
                    "mime_type": "text/plain",
                    "options": {"format": "json"},
                }
            ]
        }
    }
```

---

### 4. Services Layer (`services/`)

**Purpose:** Business logic orchestration and workflow coordination.

**MUST:**
- Implement business workflows
- Coordinate between domain entities and adapters
- Handle transactions and rollback
- Implement retry logic and error handling
- Be the ONLY layer that orchestrates adapters

**MUST NOT:**
- Contain I/O code directly (use adapters)
- Know about CLI or API specifics
- Depend on framework-specific constructs

```python
# src/{{package_name}}/services/orchestrator.py
"""
Workflow orchestration service.

This module provides the central orchestration service that coordinates
complex workflows across multiple domain entities and adapters.

The Orchestrator follows the Service Layer pattern, encapsulating
business logic and providing a clean interface for interface layers
(CLI, API) to consume.

Example:
    ```python
    from {{package_name}}.services.orchestrator import WorkflowOrchestrator
    from {{package_name}}.config.settings import Settings

    settings = Settings()
    orchestrator = WorkflowOrchestrator(settings=settings)

    result = orchestrator.execute_workflow(
        input_path=Path("data.json"),
        output_dir=Path("results/"),
    )
    print(f"Processed {result.documents_processed} documents")
    ```
"""

from __future__ import annotations

import asyncio
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

from {{package_name}}.config.settings import Settings
from {{package_name}}.domain.entities.document import Document
from {{package_name}}.domain.value_objects.content import Content
from {{package_name}}.adapters.protocols import StorageAdapter, ProcessingAdapter
from {{package_name}}.adapters.storage.filesystem import FilesystemAdapter
from {{package_name}}.adapters.http.processor import HttpProcessorAdapter
from {{package_name}}.logging.setup import get_logger
from {{package_name}}.models.schemas.responses import WorkflowResult

logger = get_logger(__name__)


@dataclass
class WorkflowOrchestrator:
    """
    Central service for orchestrating document processing workflows.

    The Orchestrator coordinates the flow of data through the system:
    1. Load input from storage adapter
    2. Create domain entities
    3. Process through processing adapter
    4. Persist results via storage adapter

    Attributes:
        settings: Application configuration.
        storage: Storage adapter for file I/O.
        processor: Processing adapter for document transformation.

    Example:
        ```python
        from {{package_name}}.services.orchestrator import WorkflowOrchestrator
        from {{package_name}}.config.settings import Settings

        orchestrator = WorkflowOrchestrator(settings=Settings())
        # Orchestrator is ready to execute workflows
        ```

    <!-- Example Test:
    >>> from {{package_name}}.services.orchestrator import WorkflowOrchestrator
    >>> from {{package_name}}.config.settings import Settings
    >>> orch = WorkflowOrchestrator(settings=Settings())
    >>> assert orch.settings is not None
    >>> assert orch.storage is not None
    -->
    """

    settings: Settings
    storage: StorageAdapter = None  # type: ignore[assignment]
    processor: ProcessingAdapter = None  # type: ignore[assignment]

    def __post_init__(self) -> None:
        """Initialize adapters with dependency injection."""
        if self.storage is None:
            self.storage = FilesystemAdapter(base_path=self.settings.storage.base_path)
        if self.processor is None:
            self.processor = HttpProcessorAdapter(
                base_url=self.settings.processor.base_url,
                timeout=self.settings.processor.timeout,
            )

    def execute_workflow(
        self,
        input_path: Path,
        output_dir: Path,
        dry_run: bool = False,
    ) -> WorkflowResult:
        """
        Execute the complete document processing workflow.

        This method orchestrates the full pipeline:
        1. Read input file(s) from storage
        2. Create Document domain entities
        3. Process each document
        4. Write results to output directory

        Args:
            input_path: Path to input file or directory.
            output_dir: Directory for output results.
            dry_run: If True, simulate without writing results.

        Returns:
            WorkflowResult containing processing statistics and status.

        Raises:
            FileNotFoundError: If input_path does not exist.
            PermissionError: If output_dir is not writable.

        Example:
            ```python
            from pathlib import Path
            from {{package_name}}.services.orchestrator import WorkflowOrchestrator
            from {{package_name}}.config.settings import Settings

            orchestrator = WorkflowOrchestrator(settings=Settings())
            result = orchestrator.execute_workflow(
                input_path=Path("input.json"),
                output_dir=Path("output/"),
                dry_run=True,  # Simulate only
            )
            print(f"Would process {result.documents_processed} documents")
            ```

        <!-- Example Test:
        >>> from {{package_name}}.services.orchestrator import WorkflowOrchestrator
        >>> from {{package_name}}.config.settings import Settings
        >>> orch = WorkflowOrchestrator(settings=Settings())
        >>> # Full workflow tested in integration tests
        >>> assert callable(orch.execute_workflow)
        -->
        """
        logger.info(
            "Starting workflow",
            input_path=str(input_path),
            output_dir=str(output_dir),
            dry_run=dry_run,
        )

        # Step 1: Load input data via storage adapter
        raw_content = self.storage.read(input_path)

        # Step 2: Create domain entity
        content = Content(text=raw_content, mime_type="application/json")
        document = Document.create(content=content, source="workflow")

        # Step 3: Process via processing adapter
        if not dry_run:
            processed_content = asyncio.run(
                self.processor.process(document.content.text)
            )
            document.mark_processed()

            # Step 4: Write results via storage adapter
            output_path = output_dir / f"{document.id}.json"
            self.storage.write(output_path, processed_content)

        logger.info(
            "Workflow completed",
            document_id=str(document.id),
            status=document.status.value,
        )

        return WorkflowResult(
            success=True,
            documents_processed=1,
            dry_run=dry_run,
        )
```

---

### 5. Adapters Layer (`adapters/`)

**Purpose:** Integration with external systems (I/O boundaries).

**MUST:**
- Implement Protocol interfaces defined in `protocols.py`
- Handle all external I/O (files, HTTP, databases)
- Provide retry logic and connection management
- Be easily mockable for testing

**MUST NOT:**
- Contain business logic
- Know about domain entities (work with primitives/DTOs)
- Be called directly from CLI/API (only through services)

```python
# src/{{package_name}}/adapters/protocols.py
"""
Adapter protocol definitions.

This module defines the abstract interfaces (Protocols) that all
adapters must implement. Using Protocols enables:

1. Dependency inversion - services depend on abstractions
2. Easy testing - mock implementations for unit tests
3. Flexibility - swap implementations without changing services

Example:
    ```python
    from {{package_name}}.adapters.protocols import StorageAdapter

    class MyCustomStorage(StorageAdapter):
        def read(self, path: Path) -> str:
            # Custom implementation
            ...

        def write(self, path: Path, content: str) -> None:
            # Custom implementation
            ...
    ```
"""

from __future__ import annotations

from pathlib import Path
from typing import Protocol, runtime_checkable


@runtime_checkable
class StorageAdapter(Protocol):
    """
    Protocol for storage operations.

    All storage adapters (filesystem, S3, etc.) must implement
    this interface to be usable by the service layer.

    Example:
        ```python
        from {{package_name}}.adapters.protocols import StorageAdapter
        from {{package_name}}.adapters.storage.filesystem import FilesystemAdapter

        # FilesystemAdapter implements StorageAdapter
        adapter: StorageAdapter = FilesystemAdapter(base_path=Path("/data"))
        content = adapter.read(Path("file.txt"))
        ```

    <!-- Example Test:
    >>> from {{package_name}}.adapters.protocols import StorageAdapter
    >>> from {{package_name}}.adapters.storage.filesystem import FilesystemAdapter
    >>> from pathlib import Path
    >>> adapter = FilesystemAdapter(base_path=Path("."))
    >>> assert isinstance(adapter, StorageAdapter)
    -->
    """

    def read(self, path: Path) -> str:
        """
        Read content from storage.

        Args:
            path: Path to the resource.

        Returns:
            Content as a string.

        Raises:
            FileNotFoundError: If resource does not exist.
            PermissionError: If resource is not readable.
        """
        ...

    def write(self, path: Path, content: str) -> None:
        """
        Write content to storage.

        Args:
            path: Path to write to.
            content: Content to write.

        Raises:
            PermissionError: If path is not writable.
        """
        ...

    def exists(self, path: Path) -> bool:
        """
        Check if a resource exists.

        Args:
            path: Path to check.

        Returns:
            True if resource exists, False otherwise.
        """
        ...


@runtime_checkable
class ProcessingAdapter(Protocol):
    """
    Protocol for document processing operations.

    Processing adapters handle the transformation of content,
    whether via local processing or external services.

    Example:
        ```python
        from {{package_name}}.adapters.protocols import ProcessingAdapter

        class LocalProcessor(ProcessingAdapter):
            async def process(self, content: str) -> str:
                return content.upper()  # Simple transformation
        ```

    <!-- Example Test:
    >>> from {{package_name}}.adapters.protocols import ProcessingAdapter
    >>> # Protocol is an abstract interface
    >>> assert hasattr(ProcessingAdapter, 'process')
    -->
    """

    async def process(self, content: str) -> str:
        """
        Process content asynchronously.

        Args:
            content: Raw content to process.

        Returns:
            Processed content.

        Raises:
            ProcessingError: If processing fails.
        """
        ...
```

---

### 6. API Layer (`api/`)

**Purpose:** HTTP API interface (like CLI, but for programmatic access).

**MUST:**
- Define FastAPI routes and endpoints
- Validate requests using Pydantic models
- Delegate ALL work to services
- Handle HTTP-specific concerns (status codes, headers)

**MUST NOT:**
- Contain business logic
- Call adapters directly
- Perform data transformations beyond serialization

```python
# src/{{package_name}}/api/routes/v1/documents.py
"""
Document API endpoints (v1).

This module provides the REST API interface for document operations.
Like the CLI, this is a thin layer that delegates to services.

Example:
    ```bash
    # Process a document via API
    curl -X POST http://localhost:8000/api/v1/documents/process \\
        -H "Content-Type: application/json" \\
        -d '{"content": "Hello, world!", "source": "api"}'
    ```
"""

from __future__ import annotations

from typing import Annotated

from fastapi import APIRouter, Depends, HTTPException, status

from {{package_name}}.api.dependencies import get_orchestrator
from {{package_name}}.models.schemas.requests import ProcessRequest
from {{package_name}}.models.schemas.responses import ProcessResponse
from {{package_name}}.services.orchestrator import WorkflowOrchestrator

router = APIRouter(prefix="/documents", tags=["documents"])


@router.post(
    "/process",
    response_model=ProcessResponse,
    status_code=status.HTTP_200_OK,
    summary="Process a document",
    description="Submit a document for processing through the workflow pipeline.",
)
async def process_document(
    request: ProcessRequest,
    orchestrator: Annotated[WorkflowOrchestrator, Depends(get_orchestrator)],
) -> ProcessResponse:
    """
    Process a document through the workflow pipeline.

    This endpoint accepts document content and processes it through
    the configured workflow, returning the processed result.

    Args:
        request: Validated process request containing content and options.
        orchestrator: Injected workflow orchestrator service.

    Returns:
        ProcessResponse containing the processing result.

    Raises:
        HTTPException: 422 for validation errors, 500 for processing failures.

    Example:
        ```python
        import httpx

        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:8000/api/v1/documents/process",
                json={"content": "Hello!", "source": "example"},
            )
            print(response.json())
        ```

    <!-- Example Test:
    >>> from {{package_name}}.api.routes.v1.documents import process_document
    >>> # API endpoint tested via integration tests
    >>> assert callable(process_document)
    -->
    """
    try:
        # Delegate to service layer - NO business logic here
        result = await orchestrator.process_document_async(request)
        return ProcessResponse.from_result(result)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        ) from e
```

---

### 7. Configuration Layer (`config/`)

**Purpose:** Centralized configuration management.

**MUST:**
- Define Pydantic Settings models
- Support environment variables, files, and CLI overrides
- Provide validation and defaults
- Be importable without side effects

**MUST NOT:**
- Perform I/O on import (lazy loading)
- Contain business logic
- Be mutable after initialization

```python
# src/{{package_name}}/config/settings.py
"""
Application configuration settings.

This module defines the configuration schema using Pydantic Settings,
supporting multiple configuration sources with clear precedence:

1. Environment variables (highest priority)
2. Configuration files (YAML/TOML)
3. Default values (lowest priority)

Example:
    ```python
    from {{package_name}}.config.settings import Settings

    # Load from environment/defaults
    settings = Settings()
    print(f"Log level: {settings.logging.level}")

    # Override with explicit values
    settings = Settings(logging=LoggingSettings(level="DEBUG"))
    ```
"""

from __future__ import annotations

from pathlib import Path
from typing import Optional

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class LoggingSettings(BaseSettings):
    """
    Logging configuration.

    Attributes:
        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
        format: Log format string or preset name.
        file: Optional file path for log output.

    Example:
        ```python
        from {{package_name}}.config.settings import LoggingSettings

        log_config = LoggingSettings(level="DEBUG")
        print(f"Logging at {log_config.level} level")
        ```

    <!-- Example Test:
    >>> from {{package_name}}.config.settings import LoggingSettings
    >>> cfg = LoggingSettings()
    >>> assert cfg.level == "INFO"
    >>> assert cfg.format == "structured"
    -->
    """

    level: str = Field(default="INFO", description="Log level")
    format: str = Field(default="structured", description="Log format")
    file: Optional[Path] = Field(default=None, description="Log file path")

    model_config = SettingsConfigDict(
        env_prefix="{{PACKAGE_NAME}}_LOG_",
        env_file=".env",
        extra="ignore",
    )


class StorageSettings(BaseSettings):
    """
    Storage configuration.

    Attributes:
        base_path: Base directory for file storage.
        max_file_size: Maximum file size in bytes.

    Example:
        ```python
        from {{package_name}}.config.settings import StorageSettings

        storage = StorageSettings(base_path=Path("/data"))
        print(f"Storage at: {storage.base_path}")
        ```

    <!-- Example Test:
    >>> from {{package_name}}.config.settings import StorageSettings
    >>> from pathlib import Path
    >>> cfg = StorageSettings()
    >>> assert isinstance(cfg.base_path, Path)
    -->
    """

    base_path: Path = Field(default=Path("."), description="Base storage path")
    max_file_size: int = Field(default=100_000_000, description="Max file size (bytes)")

    model_config = SettingsConfigDict(
        env_prefix="{{PACKAGE_NAME}}_STORAGE_",
        env_file=".env",
        extra="ignore",
    )


class ProcessorSettings(BaseSettings):
    """
    Processing service configuration.

    Attributes:
        base_url: Base URL for processing service.
        timeout: Request timeout in seconds.
        max_retries: Maximum retry attempts.

    Example:
        ```python
        from {{package_name}}.config.settings import ProcessorSettings

        proc = ProcessorSettings(base_url="http://localhost:8080")
        print(f"Processor at: {proc.base_url}")
        ```

    <!-- Example Test:
    >>> from {{package_name}}.config.settings import ProcessorSettings
    >>> cfg = ProcessorSettings()
    >>> assert cfg.timeout == 30.0
    >>> assert cfg.max_retries == 3
    -->
    """

    base_url: str = Field(default="http://localhost:8080", description="Processor URL")
    timeout: float = Field(default=30.0, description="Request timeout (seconds)")
    max_retries: int = Field(default=3, description="Max retry attempts")

    model_config = SettingsConfigDict(
        env_prefix="{{PACKAGE_NAME}}_PROCESSOR_",
        env_file=".env",
        extra="ignore",
    )


class Settings(BaseSettings):
    """
    Root application settings.

    This is the main configuration class that aggregates all
    sub-configurations into a single, validated settings object.

    Attributes:
        logging: Logging configuration.
        storage: Storage configuration.
        processor: Processing service configuration.
        debug: Enable debug mode.

    Example:
        ```python
        from {{package_name}}.config.settings import Settings

        settings = Settings()
        print(f"Debug mode: {settings.debug}")
        print(f"Log level: {settings.logging.level}")
        ```

    <!-- Example Test:
    >>> from {{package_name}}.config.settings import Settings
    >>> s = Settings()
    >>> assert s.debug is False
    >>> assert s.logging is not None
    >>> assert s.storage is not None
    >>> assert s.processor is not None
    -->
    """

    logging: LoggingSettings = Field(default_factory=LoggingSettings)
    storage: StorageSettings = Field(default_factory=StorageSettings)
    processor: ProcessorSettings = Field(default_factory=ProcessorSettings)
    debug: bool = Field(default=False, description="Enable debug mode")

    model_config = SettingsConfigDict(
        env_prefix="{{PACKAGE_NAME}}_",
        env_file=".env",
        extra="ignore",
    )
```

---

### 8. Logging Layer (`logging/`)

**Purpose:** Structured logging and observability.

**MUST:**
- Provide structured logging with context
- Support multiple output formats (JSON, human-readable)
- Enable correlation IDs for request tracing
- Be configurable via Settings

**MUST NOT:**
- Perform business logic
- Block on I/O
- Leak sensitive data

```python
# src/{{package_name}}/logging/setup.py
"""
Logging configuration and setup.

This module provides structured logging setup using structlog,
enabling rich contextual logging throughout the application.

Example:
    ```python
    from {{package_name}}.logging.setup import get_logger, configure_logging
    from {{package_name}}.config.settings import Settings

    # Configure logging (typically done once at startup)
    configure_logging(Settings())

    # Get a logger for a module
    logger = get_logger(__name__)
    logger.info("Application started", version="1.0.0")
    ```
"""

from __future__ import annotations

import logging
import sys
from typing import Any

import structlog
from structlog.types import Processor

from {{package_name}}.config.settings import Settings


def get_logger(name: str) -> structlog.stdlib.BoundLogger:
    """
    Get a configured logger instance.

    Args:
        name: Logger name (typically __name__).

    Returns:
        Configured structlog BoundLogger.

    Example:
        ```python
        from {{package_name}}.logging.setup import get_logger

        logger = get_logger(__name__)
        logger.info("Processing started", document_id="abc123")
        logger.error("Processing failed", error="timeout", retries=3)
        ```

    <!-- Example Test:
    >>> from {{package_name}}.logging.setup import get_logger
    >>> logger = get_logger("test")
    >>> assert logger is not None
    >>> assert hasattr(logger, 'info')
    >>> assert hasattr(logger, 'error')
    -->
    """
    return structlog.get_logger(name)


def configure_logging(settings: Settings) -> None:
    """
    Configure application-wide logging.

    This function should be called once at application startup
    to configure the logging infrastructure.

    Args:
        settings: Application settings containing logging config.

    Example:
        ```python
        from {{package_name}}.logging.setup import configure_logging
        from {{package_name}}.config.settings import Settings

        settings = Settings()
        configure_logging(settings)
        # Logging is now configured application-wide
        ```

    <!-- Example Test:
    >>> from {{package_name}}.logging.setup import configure_logging
    >>> from {{package_name}}.config.settings import Settings
    >>> configure_logging(Settings())
    >>> # No assertion needed - just verify no exception
    -->
    """
    # Determine processors based on format
    shared_processors: list[Processor] = [
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.ExtraAdder(),
    ]

    if settings.logging.format == "structured":
        # JSON output for production
        processors: list[Processor] = [
            *shared_processors,
            structlog.processors.JSONRenderer(),
        ]
    else:
        # Human-readable for development
        processors = [
            *shared_processors,
            structlog.dev.ConsoleRenderer(colors=True),
        ]

    structlog.configure(
        processors=processors,
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )

    # Configure stdlib logging level
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, settings.logging.level.upper()),
    )
```

---

### 9. Tests Structure

**Purpose:** Comprehensive test coverage at all layers.

```
tests/
├── conftest.py              # Shared fixtures
├── fixtures/                # Test data
│   ├── documents/
│   │   └── sample.json
│   └── config/
│       └── test_config.yaml
├── unit/                    # Fast, isolated tests
│   ├── domain/              # Domain entity tests
│   │   └── test_document.py
│   ├── models/              # Schema validation tests
│   │   └── test_requests.py
│   ├── services/            # Service logic tests (mocked adapters)
│   │   └── test_orchestrator.py
│   └── adapters/            # Adapter unit tests
│       └── test_filesystem.py
├── integration/             # Tests with real dependencies
│   ├── test_workflow.py     # Full workflow tests
│   └── test_api.py          # API endpoint tests
└── e2e/                     # End-to-end tests
    └── test_cli.py          # CLI invocation tests
```

```python
# tests/conftest.py
"""
Shared test fixtures and configuration.

This module provides pytest fixtures used across all test modules,
ensuring consistent test setup and enabling DRY testing practices.
"""

from __future__ import annotations

from pathlib import Path
from typing import Generator
from unittest.mock import AsyncMock, MagicMock

import pytest

from {{package_name}}.config.settings import Settings
from {{package_name}}.domain.entities.document import Document
from {{package_name}}.domain.value_objects.content import Content
from {{package_name}}.adapters.protocols import StorageAdapter, ProcessingAdapter


@pytest.fixture
def settings() -> Settings:
    """Provide test settings with safe defaults."""
    return Settings(debug=True)


@pytest.fixture
def sample_content() -> Content:
    """Provide a sample Content value object."""
    return Content(text="Test content for unit tests", mime_type="text/plain")


@pytest.fixture
def sample_document(sample_content: Content) -> Document:
    """Provide a sample Document entity."""
    return Document.create(content=sample_content, source="test")


@pytest.fixture
def mock_storage() -> MagicMock:
    """Provide a mock storage adapter."""
    mock = MagicMock(spec=StorageAdapter)
    mock.read.return_value = '{"test": "data"}'
    mock.exists.return_value = True
    return mock


@pytest.fixture
def mock_processor() -> AsyncMock:
    """Provide a mock processing adapter."""
    mock = AsyncMock(spec=ProcessingAdapter)
    mock.process.return_value = '{"processed": true}'
    return mock


@pytest.fixture
def tmp_data_dir(tmp_path: Path) -> Path:
    """Provide a temporary data directory for tests."""
    data_dir = tmp_path / "data"
    data_dir.mkdir()
    return data_dir
```

---

## Summary: Layer Responsibility Matrix

| Layer | Knows About | Called By | Calls |
|-------|------------|-----------|-------|
| **CLI** | Services, Config, Models (responses) | User | Services |
| **API** | Services, Config, Models | External clients | Services |
| **Services** | Domain, Adapters, Config, Models | CLI, API | Adapters, Domain |
| **Domain** | Only itself | Services | Nothing |
| **Models** | Domain (for mapping) | All layers | Nothing |
| **Adapters** | Protocols, external libs | Services | External systems |
| **Config** | Only itself | All layers | Nothing |
| **Logging** | Config | All layers | Nothing |

---

## Key Principles Recap

1. **CLI/API are thin shells** - They translate user intent into service calls. Zero business logic.

2. **Services own orchestration** - All business logic flows through services. They coordinate domain and adapters.

3. **Domain is pure** - No I/O, no frameworks, no dependencies. Just business rules and entities.

4. **Adapters are boundaries** - All external I/O goes through adapters implementing protocols.

5. **Models are contracts** - Pydantic models define the shape of data at boundaries.

6. **Configuration is declarative** - Settings are validated at startup, not scattered throughout code.

7. **Logging is structural** - Rich context, correlation IDs, machine-parseable output.

8. **Documentation is code** - Docstrings are tested, docs are versioned, examples are runnable.

This architecture ensures that when you need to add a new feature, you know exactly where each piece of code belongs, and changes in one layer don't ripple through the entire codebase.